# Copyright 2021 Alibaba Group Holding Limited. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import tensorflow as tf

from graphlearn import pywrap_graphlearn as pywrap
from graphlearn.python.errors import OutOfRangeError
from graphlearn.python.nn.dataset import Dataset as RawDataset
from graphlearn.python.nn.dataset import SubKeys
from graphlearn.python.nn.tf.data.batchgraph import BatchGraph
from graphlearn.python.nn.tf.data.egograph import EgoGraph

class Dataset(object):
  """`Dataset` object is used to convert GSL query results to Tensor format
  `Data`. It provides methods to get raw `Data` dict and `EgoGraph`s 
  composed of `Data`, and also method to get `BatchGraph`s composed of 
  `SubGraph`s when induce_func is provided(not None).

  Args:
    query: GSL query, which must contain `SubKeys` as aliases.
    induce_func: `SubGraph` inducing function, it should be either 
      induce with edge or induce with node. The induce with edge function
      requires 4 args (src, dst, src_nbrs, dst_nbrs), and the induce with 
      node function requires 2 args (src, src_nbrs). 
      This function should be overridden when you need implement 
      your own SubGraph inducing procedure.
    induce_additional_spec: A dict to describe the additional data of 
      BatchGraph which is generated by the induce_func. Each key is the name 
      of additional data, and values is a list [types, shapes], which are
      tf.dtype and tf.TensorShape instance to describe the tensor format 
      types and shapes of additional data.
    window: dataset capacity.
  """
  def __init__(self, query, window=5,                
               induce_func=None,
               induce_additional_spec=None, 
               **kwargs):
    self._dag = query
    self._induce_func = induce_func
    self._additional_spec = induce_additional_spec
    self._additional_keys = []
    if self._additional_spec is not None:
      self._additional_keys = self._additional_spec.keys()
    self._rds = RawDataset(query, window)
    self._iterator = self._make_iterator()
    # Here we call tensorflow `Iterator.get_next()` to get values 
    # in each iteration of the training loop, and then we form these values to 
    # data_dict, `EgoGraph`s and `BatchGraph`s. 
    # Note: This function must be called only once in a training loop.
    self._values = self._iterator.get_next()

  @property
  def iterator(self):
    return self._iterator
  
  def get_data_dict(self):
    """get a dict of tensor format `Data` corresponding the given query.
    Keys of the dict is the aliaes in query.
    """
    return self._rds.build_data_dict(list(self._values))

  def get_egograph(self, source, neighbors=None):
    """ Origanizes the data dict as EgoGraphs and then check and return
    the specified `EgoGraph`.
    Args:
      source(str): alias of centric vertices.
      neighbors(list of str): alias of neighbors at each hop.
        Default `None`: automatically generating the positive neighbors for
        centric vertices. It requires that each hop only has one postive
        downstream in GSL.
        Given list of string: the alias of each hop in GSL. The list must
        follow the order of traverse in GSL, and each one should be the postive
        or negative downstream for the front.
    """
    data_dict = self.get_data_dict()
    def _get_feat_spec(alias):
      node = self._dag.get_node(alias)
      decoder = node.decoder
      return decoder.feature_spec

    source_node = self._dag.get_node(source)
    nbrs = []
    hops = []
    if neighbors:
      # Use specified neighbors to construct EgoGraph.
      if not isinstance(neighbors, list):
        raise ValueError("`neighbors` should be a list of alias")
      pre = source_node
      for nbr in neighbors:
        nbr_node = self._dag.get_node(nbr)
        if not nbr_node in pre.pos_downstreams + pre.neg_downstreams:
          raise ValueError("{} is not the downstream of {}.".format(
            nbr_node.get_alias(), pre.get_alias()))
        nbrs.append(nbr_node.get_alias())
        hops.append(nbr_node.shape[-1])
        pre = nbr_node
    else:
      # Use default receptive neighbors to construct EgoGraph.
      pre = source_node
      recepts = source_node.pos_downstreams
      while recepts:
        if len(recepts) > 1:
          raise ValueError("Can't automatically find neighbors for {},"
                           " which has multiple downstreams. You should"
                           " assign specific neighbors for {}."
                           .format(pre.get_alias(), source))
        pre = recepts[0]
        nbrs.append(pre.get_alias())
        hops.append(pre.shape[-1])
        recepts = pre.pos_downstreams
    n_neighbors = []
    for idx in range(len(nbrs)):
      n_neighbors.append(hops[idx])
    return EgoGraph(data_dict[source],
                    [data_dict[nbr] for nbr in nbrs],
                    [(self._dag.get_node(v).type, _get_feat_spec(v)) for v in [source] + nbrs],
                    n_neighbors)

  def get_batchgraph(self, alias):
    """get `BatchGraph`s by given alias. Alias must be an element 
    in `SubKeys`.
    """
    pos_graph = BatchGraph.from_tensors(self._values[0:self.pos_size], 
      (self._dag.get_node(SubKeys.POS_SRC).type, 
       self._dag.get_node(SubKeys.POS_SRC).decoder),
      additional_keys = self._additional_keys)
    neg_graph = None
    if SubKeys.NEG_DST in self._dag.list_alias():
      neg_graph = BatchGraph.from_tensors(self._values[self.pos_size:],
        (self._dag.get_node(SubKeys.POS_SRC).type, 
         self._dag.get_node(SubKeys.POS_SRC).decoder),
        additional_keys = self._additional_keys)
    if (alias == SubKeys.POS_SRC) or (alias == SubKeys.POS_DST):
      return pos_graph
    elif alias == SubKeys.NEG_DST:
      return neg_graph
    else:
      raise ValueError("alias must be one of [SubKeys.POS_SRC, " 
                       "SubKeys.POS_DST, SubKeys.NEG_DST]")

  def _make_iterator(self):
    # for Data dict and EgoGraphs.
    if self._induce_func is None:
      output_types, output_shapes = self._raw_flatten_types_and_shapes()
      generator = self._raw_flatten_generator
    # for BatchGraphs.
    else: 
      output_types, output_shapes = self._batchgraph_flatten_types_and_shapes()
      generator = self._batchgraph_flatten_generator
    dataset = tf.data.Dataset.from_generator(generator,
                                             tuple(output_types),
                                             tuple(output_shapes))
    return dataset.make_initializable_iterator()

  def _raw_flatten_types_and_shapes(self):
    output_types = []
    output_shapes = []
    for alias, masks in self._rds.masks.items():
      types, shapes = self._data_types_and_shapes(alias, masks)
      output_types.extend(types)
      output_shapes.extend(shapes)
    return output_types, output_shapes

  def _raw_flatten_generator(self):
    while True:
      try:
        yield tuple(self._rds.get_flatten_values())
      except OutOfRangeError:
        break

  def _batchgraph_flatten_types_and_shapes(self):
    output_types, output_shapes = self._batchgraph_types_and_shapes()
    self.pos_size = len(output_types)
    if SubKeys.NEG_DST in self._dag.list_alias():
      neg_types, neg_shapes = self._batchgraph_types_and_shapes()
      output_types += neg_types
      output_shapes += neg_shapes
    return output_types, output_shapes

  def _batchgraph_flatten_generator(self):
    while True:
      try:
        subgraphs, neg_subgraphs = self._rds.get_subgraphs(
          induce_func=self._induce_func)
        pos_batchgraph = BatchGraph.from_graphs(subgraphs, 
                                                self._additional_keys)
        flatten_list = pos_batchgraph.flatten()
        if neg_subgraphs is not None:
          neg_batchgraph = BatchGraph.from_graphs(neg_subgraphs, 
                                                  self._additional_keys)
          flatten_list.extend(neg_batchgraph.flatten())
        yield tuple(flatten_list)
      except OutOfRangeError:
        break

  def _data_types_and_shapes(self, alias, masks):
    feat_masks, id_masks, _ = masks
    node = self._dag.get_node(alias)
    node_decoder = node.decoder

    feat_types = np.array([tf.int64, tf.float32, tf.string,
                           tf.int32, tf.float32])[feat_masks]
    feat_shapes = np.array([tf.TensorShape([None, node_decoder.int_attr_num]),
                            tf.TensorShape([None, node_decoder.float_attr_num]),
                            tf.TensorShape([None, node_decoder.string_attr_num]),
                            tf.TensorShape([None]),  # labels
                            tf.TensorShape([None])])[feat_masks] # weights

    id_types = np.array([tf.int64, tf.int64])[id_masks] # ids, dst_ids
    id_shapes = np.array([tf.TensorShape([None]), tf.TensorShape([None])])[id_masks]
    return list(feat_types) + list(id_types), list(feat_shapes) + list(id_shapes) 

  def _batchgraph_types_and_shapes(self):
    output_types, output_shapes = tuple(), tuple()
    # edge index
    output_types += tuple([tf.int32])
    output_shapes += tuple([tf.TensorShape([2, None])])
    # nodes
    node_types, node_shapes = \
      self._data_types_and_shapes(SubKeys.POS_SRC, 
                                  self._rds.masks[SubKeys.POS_SRC])
    output_types += tuple(node_types)
    output_shapes += tuple(node_shapes)
    # graph_node_offsets
    output_types += tuple([tf.int64])
    output_shapes += tuple([tf.TensorShape([None])])
    for key in self._additional_keys:
      output_types += tuple([self._additional_spec[key][0]])
      output_shapes += tuple([self._additional_spec[key][1]])
    return output_types, output_shapes